{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CORNELL_ML.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatacrc/Budget_Constrained_Bidding/blob/master/CORNELL_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7KZG35hvgjk",
        "colab_type": "text"
      },
      "source": [
        "#Machine Learning\n",
        "Source: [CORNELL CS4780](https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS)\n",
        "##Supervised Learning\n",
        "Given $D = \\{(\\overrightarrow{x_1}, y_1),\\cdots, (\\overrightarrow{x_n},y_n) \\} \\subseteq \\mathcal{X} x \\mathcal{Y}$\n",
        "\n",
        "> $\\mathcal{X} = \\mathbb{R}^d$ d dimensional space of real numbers.\n",
        "\n",
        "> $x_i = i^{th}$ sample for patient information it is number of features or for text documents it is bag of words and for images $x_i$ is pixels.\n",
        "\n",
        "Dense Vs Sparse representation: Pixels is dense representation and Bag of  Words is a Sparse representation.\n",
        "\n",
        "> $y_i \\in \\mathcal{Y}$ \n",
        "\n",
        "For Binary classification $\\mathcal{Y} = \\{0,1\\} or \\{-1,+1\\}$\n",
        "For Multiclass classification $\\mathcal{Y} = \\{0,\\cdots,k\\}$\n",
        "\n",
        "For Regression $\\mathcal{Y} = \\mathbb{R}$\n",
        "\n",
        "> Data points are sampled from iid distribution P: $(\\overrightarrow{x_i}, y_i) \\sim P.$ \n",
        "P is unknown otherwise we can compute P(y|x).\n",
        "\n",
        "> iid independent and identically distributed means knowing one does not tell you anything about the others\n",
        "\n",
        "###Hypothesis class\n",
        "$h\\in\\mathcal{H}$\n",
        "\n",
        "Examples: Decision Trees, Linear Classifiers, Support Vector Machines, kNN, Artificial Neural Networks Etc.\n",
        "\n",
        "###Loss Functions\n",
        "How well $h$ works on data $D.$\n",
        "\n",
        "Zero-one loss = $Loss(h;D) = \\frac{1}{n}\\sum\\limits_{(x_i,y_i)\\leftarrow D} \\delta(h(x_i) \\neq y_i)$\n",
        "\n",
        "Squared loss:  $Loss(h;D) = \\frac{1}{n}\\sum\\limits_{(x_i,y_i)\\leftarrow D} (h(x_i) - y_i)^2$\n",
        "\n",
        "Absolute loss: $Loss(h;D) = \\frac{1}{n}\\sum\\limits_{(x_i,y_i)\\leftarrow D} |h(x_i) - y_i|$\n",
        "\n",
        "When do you use Squared loss over Absolute loss?\n",
        "When the outliers are more squared loss amplifies the loss.\n",
        "\n",
        "###Generalization Loss\n",
        "\n",
        "We can not compute it but we can approximate it by splitting the data into training, validation, and test datasets.\n",
        "\n",
        "Using weak law of large numbers: Average of the random variable becomes the expected value in the limit.\n",
        "The **Weak Law of Large Numbers**, also known as Bernoulli's theorem, states that if you have a sample of independent and identically distributed random variables, as the sample size grows larger, the sample mean will tend toward the population mean.\n",
        "\n",
        "$\\mathbb{E}[l(h;(\\overrightarrow x, y))]_{(\\overrightarrow{x_i}, y_i) \\sim P} = \\lim\\limits_{n\\rightarrow \\infty} \\frac{1}{n}\\sum \\limits_{i=1}^nl(h,(x_i,y_i))$\n",
        "\n",
        "Each Machine Learning makes some assumptions about the data. Try different algorithms and use the one which works well with the data.\n",
        "\n",
        "## kNN\n",
        "invented in 1967\n",
        "The assumption kNN makes is the data points that are close together belongs to the same output class. usually k is odd 3,5 etc. kNN is as good as its distance measure. Common distance metrics is **Minkowski distance**.\n",
        "\n",
        "$dist(x,z) = (\\sum\\limits_{r=1}^o'|[x]_r-[z]_r|^p)^\\frac{1}{p}$\n",
        "\n",
        "p=1 Manhattan distance\n",
        "\n",
        "p=2 Euclidean distance\n",
        "\n",
        "p=$\\infty$ Chebyshev distance: Max difference between larger difference.\n",
        "\n",
        "\n",
        "Test point x\n",
        "\n",
        ">$S_x \\subseteq D\\ s.t. |S_x| = k$ \n",
        "\n",
        ">$\\forall (x',y') \\in D\\setminus S_x$\n",
        "\n",
        ">$dist(x,x') \\geq {\\max\\limits_{(x\",y\")\\in S_x}} dist(x,x\")$\n",
        "\n",
        "##Curse of Dimensionality\n",
        "\n",
        "For high dimensional spaces if you don't make any assumptions of the data, kNN does not work.\n",
        "\n",
        "$l^d \\approx (\\frac{k}{n})^{\\frac{1}{d}}$\n",
        "\n",
        "d|l\n",
        "---|---\n",
        "2| 0.1\n",
        "10|0.63\n",
        "100|0.955\n",
        "1000|0.9954\n",
        "\n",
        "As d becomes larger then it is difficult to find points closeby. But in the high dimensional space if your data lies in some sub-dimensional space(intrisic dimensionality) or low-dimensional manifold  (e.g. images) then kNN still works. Underlying manifold is low dimensional, like euclidean like small section of earth looks flat. \n",
        "\n",
        "**PCA** finds the new coordinate system where your data has large variance and drop out all other dimensions where you don't have data.\n",
        "\n",
        "What are the true dimensionality of your data?\n",
        "Even though the pixels have 10000 dimensions you don't need all those dimensions to find out a face or not.\n",
        "\n",
        "Disadvantages:\n",
        "1. As n could be $\\infty$ collect more data, why kNN could be a bad algorithm? \n",
        "\n",
        "It is going to take lot of time to compute distances. $O(n^d)$ Inference time is huge.\n",
        "1. We have to store every single data point\n",
        "\n",
        "###Perceptron\n",
        "Perceptron makes very restrictive assumption on the decision boundary and It has to be a hyperplane.\n",
        "For Linearly separable data, binary class data. In high dimensional space there always exists the hyperplane that separates the classes.\n",
        "\n",
        "Hyperplane is one dimension lower than the data for 2d inputs hyperplane is 1d.\n",
        "\n",
        "$\\mathcal{H}=\\{x:w^Tx+b=0\\}$\n",
        "\n",
        "Computing which side of the hyperplane the data point lies is always constant time. $sign(w^T+x)$\n",
        "\n",
        "$\\mathcal{Y}=\\{-1, +1\\}$\n",
        "\n",
        "$\\overrightarrow x \\rightarrow{\\overrightarrow x \\choose 1}$\n",
        "$\\overrightarrow w \\rightarrow{\\overrightarrow w \\choose b}$\n",
        "$\\mathcal{H}=\\{x:w^Tx=0\\}$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Advantage:\n",
        "* It is enough to store the weights and bias for determining the output class.\n",
        "\n",
        "Disadvantage:\n",
        "* It is not possible to find a hyperplane if the data is alteranting +'s and -'s. Additional data doesn't help.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfRw5zv3y4ED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perceptron Algorithm\n",
        "\n",
        "$\\overrightarrow w = \\overrightarrow0$\n",
        "while True:\n",
        "  m = 0\n",
        "  for (x,y) in D:\n",
        "    if ywTx <= 0:\n",
        "      w := w + yx\n",
        "      m += 1 \n",
        "  if m == 0:\n",
        "    break \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv461rfU8KJu",
        "colab_type": "text"
      },
      "source": [
        "If we know the distribution P(X,Y), then we can have optimal classifier. \n",
        "\n",
        "We can do empirical approximation of the distribution from the data. Then we can use Bayes Optimal classifier. All of machine learning is all about P(Y|X).\n",
        "\n",
        "Discriminative Learning\n",
        "* P(X,Y) = P(Y|X)P(X)\n",
        "\n",
        "Generative Learning\n",
        "* P(X,Y) = P(X|Y)P(Y)\n",
        "\n",
        "## Maximum Likelihood Estimate(MLE)\n",
        "\n",
        "Frequentist statistics: Which parameter $\\theta$ makes our data most likely.\n",
        "\n",
        "MLE $P(D;\\theta)$\n",
        "\n",
        "$\\theta = {\\arg\\max\\limits_{\\theta}} P(D;\\theta) = {\\arg\\max\\limits_{\\theta}} P_\\theta(D)$\n",
        "\n",
        "$P(D;\\theta) = {{n_H+n_T} \\choose n_H} \\theta^{n_H}(1-\\theta)^{n_T}$\n",
        "\n",
        "$\\theta = {\\arg\\max\\limits_\\theta}\\ log[{{n_H+n_T} \\choose n_H} \\theta^{n_H}(1-\\theta)^{n_T}]$\n",
        "\n",
        "Take derivative and equate to zero, we get:\n",
        "\n",
        "$\\theta = \\frac{n_H}{n_H+n_T}$\n",
        "\n",
        "When MLE can fail?\n",
        "If you only see all tails then prob. of seeing heads = 0. This could happen if you have very little data.\n",
        "\n",
        "**Smoothing** Instead we say we saw m tosses of H and m tosses of T. 50% prob of seeing H or T. Then we compute the prob. using the prior belief.\n",
        "\n",
        "##Maximum A Posteriori (MAP)\n",
        "Bayesian Statistics: Given data which $\\theta$ parameters most likely.\n",
        "\n",
        "$P(D/\\theta)$ means $\\theta$ no longer a parameter rather a random variable.\n",
        "\n",
        "$P(D/\\theta) \\leftarrow likelihood$\n",
        "\n",
        "$P(\\theta) \\leftarrow prior$\n",
        "\n",
        "$P(\\theta/D) \\leftarrow posterior$\n",
        "\n",
        "$P(\\theta/D) = \\frac{P(D/\\theta)P(\\theta)}{P(D)}$\n",
        "\n",
        "$Posterior = \\frac{Likelihood.Prior}{Normalizer}$\n",
        "\n",
        "$P(\\theta)$ follows Dirichlet distribution\n",
        "\n",
        "$P(heads/\\theta) = \\int\\limits_{\\theta}P(heads,\\theta/D) d\\theta = \\int\\limits_{\\theta}P(heads/\\theta,D) d\\theta$\n",
        "\n",
        "MAP: $\\theta = {\\arg\\max\\limits_{\\theta}} P(\\theta|D)$\n",
        "\n",
        "$P(y|X=x) = \\int\\limits_{\\theta}P(y|\\theta)P(\\theta|D)d\\theta$\n",
        "\n",
        "MAP is good if you start with a good prior. If you don't know anything then we can start with uninformed prior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmNh9pzH5VXd",
        "colab_type": "text"
      },
      "source": [
        "##Naive Bayes\n",
        "\n",
        "Naive Bayes makes an assumption that features are independent of each other given the output label.\n",
        "\n",
        "$P(\\overrightarrow X=\\overrightarrow x|Y=y) = \\Pi_{\\alpha=0}^d P([X]_\\alpha=x_\\alpha|Y=y)$\n",
        "\n",
        "$P(Y=y|X=x) = \\frac{P(X=x|Y=y)P(Y=y)}{P(X=x)}$\n",
        "\n",
        "$h(\\overrightarrow x)=\\arg\\max\\limits_{y} P(y|\\overrightarrow x) = \\arg\\max\\limits_y \\frac{P(\\overrightarrow x|y)P(y)}{z} = \\arg\\max\\limits_yP(y) \\Pi_{\\alpha=0}^d P([\\overrightarrow x]_\\alpha|Y=y)$\n",
        "\n"
      ]
    }
  ]
}